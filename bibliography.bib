@inproceedings{robertsBayesianIndependentComponent2005,
  title = {Bayesian {{Independent Component Analysis}} with {{Prior Constraints}}: {{An Application}} in {{Biosignal Analysis}}},
  shorttitle = {Bayesian {{Independent Component Analysis}} with {{Prior Constraints}}},
  booktitle = {Deterministic and {{Statistical Methods}} in {{Machine Learning}}},
  author = {Roberts, Stephen and Choudrey, Rizwan},
  editor = {Winkler, Joab and Niranjan, Mahesan and Lawrence, Neil},
  year = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {159--179},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11559887_10},
  abstract = {In many data-driven machine learning problems it is useful to consider the data as generated from a set of unknown (latent) generators or sources. The observations we make are then taken to be related to these sources through some unknown functionaility. Furthermore, the (unknown) number of underlying latent sources may be different to the number of observations and hence issues of model complexity plague the analysis. Recent developments in Independent Component Analysis (ICA) have shown that, in the case where the unknown function linking sources to observations is linear, data decomposition may be achieved in a mathematically elegant manner. In this paper we extend the general ICA paradigm to include a very flexible source model and prior constraints and argue that for particular biomedical signal processing problems (we consider EEG analysis) we require the constraint of positivity in the mixing process.},
  isbn = {978-3-540-31728-9},
  langid = {english},
  keywords = {biosignal analysis,Independent component analysis,prior constraints,variational Bayes},
  file = {/Users/tedgro/Zotero/storage/SD8MEYGN/Roberts and Choudrey - 2005 - Bayesian Independent Component Analysis with Prior.pdf}
}

@article{piironenSparsityInformationRegularization2017,
  title = {Sparsity Information and Regularization in the Horseshoe and Other Shrinkage Priors},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  issn = {1935-7524},
  doi = {10.1214/17-EJS1337SI},
  url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full},
  urldate = {2023-03-13},
  abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/ZUX4LE4Q/Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf}
}

@article{tanIndependentComponentAnalysis2020,
  title = {Independent Component Analysis of {{E}}. Coli's Transcriptome Reveals the Cellular Processes That Respond to Heterologous Gene Expression},
  author = {Tan, Justin and Sastry, Anand V. and Fremming, Karoline S. and Bj{\o}rn, Sara P. and Hoffmeyer, Alexandra and Seo, Sangwoo and Voldborg, Bj{\o}rn G. and Palsson, Bernhard O.},
  year = {2020},
  month = sep,
  journal = {Metabolic Engineering},
  volume = {61},
  pages = {360--368},
  issn = {1096-7176},
  doi = {10.1016/j.ymben.2020.07.002},
  url = {https://www.sciencedirect.com/science/article/pii/S1096717620301117},
  urldate = {2022-11-29},
  abstract = {Achieving the predictable expression of heterologous genes in a production host has proven difficult. Each heterologous gene expressed in the same host seems to elicit a different host response governed by unknown mechanisms. Historically, most studies have approached this challenge by manipulating the properties of the heterologous gene through methods like codon optimization. Here we approach this challenge from the host side. We express a set of 45 heterologous genes in the same Escherichia coli strain, using the same expression system and culture conditions. We collect a comprehensive RNAseq set to characterize the host's transcriptional response. Independent Component Analysis of the RNAseq data set reveals independently modulated gene sets (iModulons) that characterize the host response to heterologous gene expression. We relate 55\% of variation of the host response to: Fear vs Greed (16.5\%), Metal Homeostasis (19.0\%), Respiration (6.0\%), Protein folding (4.5\%), and Amino acid and nucleotide biosynthesis (9.0\%). If these responses can be controlled, then the success rate with predicting heterologous gene expression should increase.},
  langid = {english},
  keywords = {Big data,Heterologous gene expression,Host cell response,Independent component analysis,Metabolic burden,Plasmid},
  file = {/Users/tedgro/Zotero/storage/444REBT5/Tan et al. - 2020 - Independent component analysis of E. coli's transc.pdf;/Users/tedgro/Zotero/storage/3CKUYAPQ/S1096717620301117.html}
}

@article{hyvarinenIndependentComponentAnalysis2000,
  title = {Independent Component Analysis: Algorithms and Applications},
  shorttitle = {Independent Component Analysis},
  author = {Hyv{\"a}rinen, A. and Oja, E.},
  year = {2000},
  month = jun,
  journal = {Neural Networks},
  volume = {13},
  number = {4},
  pages = {411--430},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(00)00026-5},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
  urldate = {2022-11-28},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  langid = {english},
  keywords = {Blind signal separation,Factor analysis,Independent component analysis,Projection pursuit,Representation,Source separation},
  file = {/Users/tedgro/Zotero/storage/CDII4FS4/Hyv√§rinen and Oja - 2000 - Independent component analysis algorithms and app.pdf;/Users/tedgro/Zotero/storage/DZMUBZNS/S0893608000000265.html}
}

@article{zhaoMisuseRPKMTPM2020,
  title = {Misuse of {{RPKM}} or {{TPM}} Normalization When Comparing across Samples and Sequencing Protocols},
  author = {Zhao, Shanrong and Ye, Zhan and Stanton, Robert},
  year = {2020},
  month = aug,
  journal = {RNA},
  volume = {26},
  number = {8},
  pages = {903--909},
  issn = {1355-8382, 1469-9001},
  doi = {10.1261/rna.074922.120},
  url = {http://rnajournal.cshlp.org/lookup/doi/10.1261/rna.074922.120},
  urldate = {2023-04-12},
  abstract = {In recent years, RNA-sequencing (RNA-seq) has emerged as a powerful technology for transcriptome profiling. For a given gene, the number of mapped reads is not only dependent on its expression level and gene length, but also the sequencing depth. To normalize these dependencies, RPKM (reads per kilobase of transcript per million reads mapped) and TPM (transcripts per million) are used to measure gene or transcript expression levels. A common misconception is that RPKM and TPM values are already normalized, and thus should be comparable across samples or RNA-seq projects. However, RPKM and TPM represent the relative abundance of a transcript among a population of sequenced transcripts, and therefore depend on the composition of the RNA population in a sample. Quite often, it is reasonable to assume that total RNA concentration and distributions are very close across compared samples. Nevertheless, the sequenced RNA repertoires may differ significantly under different experimental conditions and/or across sequencing protocols; thus, the proportion of gene expression is not directly comparable in such cases. In this review, we illustrate typical scenarios in which RPKM and TPM are misused, unintentionally, and hope to raise scientists' awareness of this issue when comparing them across samples or different sequencing protocols.},
  langid = {english},
  file = {/Users/tedgro/Zotero/storage/C3JFMV8M/Zhao et al. - 2020 - Misuse of RPKM or TPM normalization when comparing.pdf}
}
